{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperas_source_sai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUdzyFqGZxQ/zLTd28aysh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SachitNayak/Website-Changes-Tracker/blob/master/hyperas_source_sai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaFPFcsPUBHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from statistics import stdev\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from getpass import getpass\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import *\n",
        "from keras.callbacks import ModelCheckpoint, LambdaCallback, EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.optimizers import SGD\n",
        "from decimal import *\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from hyperas import optim\n",
        "except Exception as e:\n",
        "    !pip3 install hyperas\n",
        "\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh7Gs1rahorl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.getcwd().endswith('content'):\n",
        "    os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lHuuSl5ZM1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"/content/arunima_re\"):\n",
        "    pass_phrase = getpass(prompt=\"Please provide your github passphrase: \")\n",
        "    os.system(\"git clone https://'SachitNayak':'\"+pass_phrase+\"'@github.com/SachitNayak/arunima_re.git\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvbLuLtEwOYU",
        "colab_type": "code",
        "outputId": "bb92070c-6ffd-4f56-b8a0-3b4745339dfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "% run ./hyperas_test.ipynb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- 1.6027166843414307 seconds ---\n",
            "Shape of long_shot matrix is:  (9523, 39)\n",
            "Shape of sums_col is:  (9523, 1)\n",
            "Shape of max_range col is:  (9523, 1)\n",
            "Shape of std_gaps col is:  (9523, 1)\n",
            "Shape of actual_vals is:  (9523, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jlakB3oU67M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd_X_filename = \"pd_X_store.hdf5\"\n",
        "y_filename = \"y_store.npy\"\n",
        "\n",
        "if not os.path.exists(pd_X_filename) or not os.path.exists(y_filename):\n",
        "    print(\"Data points files not found.\")\n",
        "    exit(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMJlTeCLTyHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data():\n",
        "    \"\"\"\n",
        "    Data providing function:\n",
        "\n",
        "    This function is separated from create_model() so that hyperopt\n",
        "    won't reload data for each evaluation run.\n",
        "    \"\"\"\n",
        "    pd_X_filename = \"pd_X_store.hdf5\"\n",
        "    y_filename = \"y_store.npy\"\n",
        "    pd_X = pd.read_hdf(pd_X_filename)\n",
        "    y = np.load(y_filename)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(pd_X, y, test_size=0.33, random_state=108)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def create_model(x_train, y_train, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Model providing function:\n",
        "\n",
        "    Create Keras model with double curly brackets dropped-in as needed.\n",
        "    Return value has to be a valid python dictionary with two customary keys:\n",
        "        - loss: Specify a numeric evaluation metric to be minimized\n",
        "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
        "    The last one is optional, though recommended, namely:\n",
        "        - model: specify the model just created so that we can later use it again.\n",
        "    \"\"\"\n",
        "    actual_cols_size = pd_X.shape[1]\n",
        "    actual_input = Input(shape=(actual_cols_size,), name='actual_input')\n",
        "\n",
        "    d_1 = Dense({{choice(range(43,501))}}, activation={{choice(['linear','relu', 'sigmoid'])}})(actual_input)\n",
        "    b_1 = BatchNormalization()(d_1)\n",
        "    d_2 = Dense({{choice(range(43,501))}}, activation={{choice(['linear','relu', 'sigmoid'])}})(b_1)\n",
        "    b_2 = BatchNormalization()(d_2)\n",
        "    d_3 = Dense({{choice(range(43,501))}}, activation={{choice(['linear','relu', 'sigmoid'])}})(b_2)\n",
        "    b_3 = BatchNormalization()(d_3)\n",
        "    d_4 = Dense({{choice(range(43,501))}}, activation={{choice(['linear','relu', 'sigmoid'])}})(b_3)\n",
        "\n",
        "    model_output = Dense(39, activation='sigmoid')(d_4)\n",
        "    model = Model(inputs=[actual_input], outputs=[model_output])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
        "                    optimizer='adam')\n",
        "\n",
        "    result = model.fit(X_train, y_train,\n",
        "              batch_size=128,\n",
        "              epochs=2,\n",
        "              verbose=2,\n",
        "              validation_split=0.1)\n",
        "    #get the highest validation accuracy of the training epochs\n",
        "    validation_acc = np.amax(result.history['val_acc']) \n",
        "    print('Best validation acc of epoch:', validation_acc)\n",
        "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjaaTs9h3HEs",
        "colab_type": "code",
        "outputId": "f36a8737-ca94-4d1c-827f-34e234304f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    best_run, best_model = optim.minimize(model=create_model,\n",
        "                                          data=data,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=10,\n",
        "                                          trials=Trials(),\n",
        "                                          notebook_name='hyperas_source', \n",
        "                                          eval_space=True)\n",
        "    print(\"Evalutation of best performing model:\")\n",
        "    print(best_model.evaluate(X_test, y_test))\n",
        "    print(\"Best performing model chosen hyper-parameters:\")\n",
        "    print(best_run)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "from __future__ import print_function\n",
            "\n",
            "try:\n",
            "    import os\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import csv\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import re\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import sys\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from collections import defaultdict, Counter\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from copy import deepcopy\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import random\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from statistics import stdev\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from multiprocessing.pool import ThreadPool\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.preprocessing import StandardScaler\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import time\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import warnings\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import tensorflow as tf\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import json\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential, Model\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import *\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.callbacks import ModelCheckpoint, LambdaCallback, EarlyStopping\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import load_model\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import model_from_json\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.optimizers import SGD\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from decimal import *\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import drive\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import np_utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'Dense': hp.choice('Dense', range(43,501)),\n",
            "        'activation': hp.choice('activation', ['linear','relu', 'sigmoid']),\n",
            "        'Dense_1': hp.choice('Dense_1', range(43,501)),\n",
            "        'activation_1': hp.choice('activation_1', ['linear','relu', 'sigmoid']),\n",
            "        'Dense_2': hp.choice('Dense_2', range(43,501)),\n",
            "        'activation_2': hp.choice('activation_2', ['linear','relu', 'sigmoid']),\n",
            "        'Dense_3': hp.choice('Dense_3', range(43,501)),\n",
            "        'activation_3': hp.choice('activation_3', ['linear','relu', 'sigmoid']),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "  1: \n",
            "  2: \"\"\"\n",
            "  3: Data providing function:\n",
            "  4: \n",
            "  5: This function is separated from create_model() so that hyperopt\n",
            "  6: won't reload data for each evaluation run.\n",
            "  7: \"\"\"\n",
            "  8: pd_X_filename = \"pd_X_store.hdf5\"\n",
            "  9: y_filename = \"y_store.npy\"\n",
            " 10: pd_X = pd.read_hdf(pd_X_filename)\n",
            " 11: y = np.load(y_filename)\n",
            " 12: X_train, X_test, y_train, y_test = train_test_split(pd_X, y, test_size=0.33, random_state=108)\n",
            " 13: \n",
            " 14: \n",
            " 15: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     \"\"\"\n",
            "   4:     Model providing function:\n",
            "   5: \n",
            "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
            "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
            "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
            "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
            "  10:     The last one is optional, though recommended, namely:\n",
            "  11:         - model: specify the model just created so that we can later use it again.\n",
            "  12:     \"\"\"\n",
            "  13:     actual_cols_size = pd_X.shape[1]\n",
            "  14:     actual_input = Input(shape=(actual_cols_size,), name='actual_input')\n",
            "  15: \n",
            "  16:     d_1 = Dense(space['Dense'], activation=space['activation'])(actual_input)\n",
            "  17:     b_1 = BatchNormalization()(d_1)\n",
            "  18:     d_2 = Dense(space['Dense_1'], activation=space['activation_1'])(b_1)\n",
            "  19:     b_2 = BatchNormalization()(d_2)\n",
            "  20:     d_3 = Dense(space['Dense_2'], activation=space['activation_2'])(b_2)\n",
            "  21:     b_3 = BatchNormalization()(d_3)\n",
            "  22:     d_4 = Dense(space['Dense_3'], activation=space['activation_3'])(b_3)\n",
            "  23: \n",
            "  24:     model_output = Dense(39, activation='sigmoid')(d_4)\n",
            "  25:     model = Model(inputs=[actual_input], outputs=[model_output])\n",
            "  26: \n",
            "  27:     model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
            "  28:                     optimizer='adam')\n",
            "  29: \n",
            "  30:     result = model.fit(X_train, y_train,\n",
            "  31:               batch_size=128,\n",
            "  32:               epochs=2,\n",
            "  33:               verbose=2,\n",
            "  34:               validation_split=0.1)\n",
            "  35:     #get the highest validation accuracy of the training epochs\n",
            "  36:     validation_acc = np.amax(result.history['val_acc']) \n",
            "  37:     print('Best validation acc of epoch:', validation_acc)\n",
            "  38:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
            "  39: \n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 3s - loss: 0.4182 - acc: 0.8452 - val_loss: 0.3775 - val_acc: 0.8719\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.3716 - acc: 0.8718 - val_loss: 0.3752 - val_acc: 0.8720\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.8719556307344227\n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 3s - loss: 0.6229 - acc: 0.6710 - val_loss: 0.4468 - val_acc: 0.8609\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.3876 - acc: 0.8710 - val_loss: 0.3898 - val_acc: 0.8702\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.8701872909704346\n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 3s - loss: 0.5336 - acc: 0.7547 - val_loss: 0.4328 - val_acc: 0.8574\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.4064 - acc: 0.8673 - val_loss: 0.4006 - val_acc: 0.8701\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.8701069192452864\n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 4s - loss: 0.4578 - acc: 0.8222 - val_loss: 0.4009 - val_acc: 0.8705\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.3722 - acc: 0.8716 - val_loss: 0.3936 - val_acc: 0.8711\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.871111654936333\n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 4s - loss: 0.6511 - acc: 0.6423 - val_loss: 0.4797 - val_acc: 0.8524\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.3942 - acc: 0.8709 - val_loss: 0.3877 - val_acc: 0.8714\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.8714331764038827\n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 4s - loss: 0.4181 - acc: 0.8464 - val_loss: 0.3796 - val_acc: 0.8718\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.3741 - acc: 0.8718 - val_loss: 0.3773 - val_acc: 0.8718\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.8717948794364929\n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 4s - loss: 0.6722 - acc: 0.6119 - val_loss: 0.5653 - val_acc: 0.7944\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.4347 - acc: 0.8620 - val_loss: 0.3824 - val_acc: 0.8714\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.871392980825191\n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 4s - loss: 0.4432 - acc: 0.8279 - val_loss: 0.3785 - val_acc: 0.8718\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.3716 - acc: 0.8718 - val_loss: 0.3758 - val_acc: 0.8718\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.8717948794364929\n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 5s - loss: 0.6664 - acc: 0.6168 - val_loss: 0.5119 - val_acc: 0.8208\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.4076 - acc: 0.8676 - val_loss: 0.3939 - val_acc: 0.8709\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.8709107117966799\n",
            "Train on 5742 samples, validate on 638 samples\n",
            "Epoch 1/2\n",
            " - 5s - loss: 0.4632 - acc: 0.8133 - val_loss: 0.3928 - val_acc: 0.8710\n",
            "\n",
            "Epoch 2/2\n",
            " - 0s - loss: 0.3750 - acc: 0.8718 - val_loss: 0.3842 - val_acc: 0.8715\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.8715135686823567\n",
            "100%|██████████| 10/10 [00:53<00:00,  5.84s/it, best loss: -0.8719556307344227]\n",
            "Evalutation of best performing model:\n",
            "3143/3143 [==============================] - 0s 74us/step\n",
            "[0.3759525078157691, 0.8718519862806422]\n",
            "Best performing model chosen hyper-parameters:\n",
            "{'Dense': 237, 'Dense_1': 180, 'Dense_2': 180, 'Dense_3': 430, 'activation': 'sigmoid', 'activation_1': 'linear', 'activation_2': 'relu', 'activation_3': 'sigmoid'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq4rGkjdYD4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# % cd ../\n",
        "# % rm -rf arunima_re/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}